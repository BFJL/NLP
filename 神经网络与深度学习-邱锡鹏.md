# 神经网络与深度学习-邱锡鹏

# 第2章 机器学习概述

### 2.2.2.1 损失函数

**0-1** **损失函数**

![image-20210525142536974](/images/image-20210525142536974.png)

**平方损失函数**

![image-20210525142635658](/images/image-20210525142635658.png)

**交叉熵损失函数**

交叉熵损失函数（Cross-Entropy Loss Function）一般用于分类问题．假设样本的标签 𝑦 ∈ {1, ⋯ , 𝐶} 为离散的类别，模型 𝑓(𝒙; 𝜃) ∈ [0, 1]𝐶的输出为类别标签的条件概率分布，即

![image-20210525142805524](/images/image-20210525142805524.png)

并满足

![image-20210525142817757](/images/image-20210525142817757.png)

我们可以用一个𝐶 维的one-hot向量𝒚来表示样本标签．假设样本的标签为𝑘，那么标签向量𝒚只有第𝑘维的值为1，其余元素的值都为0．标签向量𝒚可以看作样本标签的真实条件概率分布𝑝𝑟 (𝒚|𝒙)，即第𝑐维（记为𝑦𝑐，1 ≤ 𝑐 ≤ 𝐶）是类别为 𝑐 的真实条件概率．假设样本的类别为 

𝑘，那么它属于第 𝑘 类的概率为 1，属于其他类的概率为0．对于两个概率分布，一般可以用交叉熵来衡量它们的差异． 标签的真实分布𝒚和模型预测分布𝑓(𝒙; 𝜃)之间的交叉熵为

![image-20210525142959827](/images/image-20210525142959827.png)

如果y为one-hot向量，也可以写为

![image-20210525143131780](/images/image-20210525143131780.png)

**Hinge** **损失函数**

对于二分类问题，假设 𝑦 的取值为 {−1, +1}，𝑓(𝒙; 𝜃) ∈ ℝ．Hinge损失函数（Hinge Loss Function）为

![image-20210525143215743](/images/image-20210525143215743.png)

### **2.2.2.2** **风险最小化准则**

经验风险（Empirical Risk），即在训练集上的平均损失：

![image-20210525143311725](/images/image-20210525143311725.png)

因此，一个切实可行的学习准则是找到一组参数𝜃∗ 使得经验风险最小，即 

![image-20210525143354450](/images/image-20210525143354450.png)

这就是经验风险最小化（Empirical Risk Minimization，ERM）准则。

**过拟合**

过拟合问题往往是由于训练数据少和噪声以及模型能力强等原因造成的。

为了解决过拟合问题，一般在经验风险最小化的基础上再引入参数的正则化

（Regularization）来限制模型能力，使其不要过度地最小化经验风险． 

这种准则就是结构风险最小化（Structure Risk Minimization，SRM）准则：

![image-20210525143612181](/images/image-20210525143612181.png)

其中‖𝜃‖是ℓ2 范数的正则化项，用来减少参数空间，避免过拟合；𝜆用来控制正则

化的强度。

**防止过拟合**

- 正则化
- 提前停止
- 限制模型的复杂度

和过拟合相反的一个概念是**欠拟合（Underfitting）**，即模型不能很好地拟

合训练数据，在训练集上的错误率比较高．欠拟合一般是由于模型能力不足造成

的．

可以将机器学习看作一个从有限、高维、有噪声的数据上得到更一般性规律的泛化问题．

### **2.2.3** **优化算法**

在确定了训练集 𝒟、假设空间 ℱ 以及学习准则后，如何找到最优的模型𝑓(𝒙, 𝜃∗) 就成了一个最优化（Optimization）问题．机器学习的训练过程其实就是最优化问题的求解过程．

### **2.2.3.1** **梯度下降法**

最简单、常用的优化算法就是梯度下降法，即首先初始化参数𝜃0，然后按下面的迭代公式来计算训练集𝒟 上风险函数的最小值：

![image-20210525144000493](/images/image-20210525144000493.png)

### **2.2.3.3** **随机梯度下降法**

梯度下降法中，目标函数是整个训练集上的风险函数，这种方式称为批量梯度下降法（Batch Gradient Descent，BGD）．批量梯度下降法在每次迭代时需要计算每个样本上损失函数的梯度并求和．当训练集中的样本数量𝑁 很大时，空间复杂度比较高，每次迭代的计算开销也很大．

为了减少每次迭代的计算复杂度，我们也可以在每次迭代时只采集一个样本，计算这个样本损失函数的梯度并更新参数，即随机梯度下降法（Stochastic Gradient Descent，SGD）．

随机梯度下降法也叫作增量梯度下降法． 

批量梯度下降和随机梯度下降之间的区别在于，每次迭代的优化目标是对所有样本的平均损失函数还是对单个样本的损失函数．由于随机梯度下降实现简单，收敛速度也非常快，因此使用非常广泛．随机梯度下降相当于在批量梯度下降的梯度上引入了随机噪声．在非凸优化问题中，随机梯度下降更容易逃离局部最优点。

### **2.2.3.4** **小批量梯度下降法**

随机梯度下降法的一个缺点是无法充分利用计算机的并行计算能力．小批量梯度下降法（Mini-Batch Gradient Descent）是批量梯度下降和随机梯度下降的折中．每次迭代时，我们随机选取一小部分训练样本来计算梯度并更新参数，这样既可以兼顾随机梯度下降法的优点，也可以提高训练效率．

![image-20210525144517925](/images/image-20210525144517925.png)

### **2.3.1** **参数学习**

### **2.3.1.1** **经验风险最小化**

于线性回归的标签𝑦和模型输出都为连续的实数值， 因此平方损失函数非常合适衡量真实标签和预测标签之间的差异．

根据经验风险最小化准则，训练集𝒟 上的经验风险定义为

![image-20210525151515960](/images/image-20210525151515960.png)

风险函数ℛ(𝒘)是关于𝒘的凸函数，其对𝒘的偏导数为

![image-20210525151539321](/images/image-20210525151539321.png)

![image-20210525151555999](/images/image-20210525151555999.png)

这种求解线性回归参数的方法也叫最小二乘法（Least Square Method，LSM）．

**2.3.1.2** **结构风险最小化**

TODO

**2.3.1.3** **最大似然估计**

**2.3.1.4** **最大后验估计**

### 2.4 偏差-方差分解

拟合能力强的模型一般复杂度会比较高，容易导致过拟合．相反，如果限制模型的

复杂度，降低其拟合能力，又可能会导致欠拟合．因此，如何在模型的拟合能力和

复杂度之间取得一个较好的平衡，对一个机器学习算法来讲十分重要．偏差-方

差分解（Bias-Variance Decomposition）为我们提供了一个很好的分析和指导

工具．

偏差（Bias），是指一个模型在不同训练集上的平均性能和最优模型的差异，可以用来衡量一个模型的拟合能力．（低偏差靠近中心）

方差（Variance），是指一个模型在不同训练集上的差异，可以用来衡量一个模型是否容易过拟合．（低方差聚集）

方差一般会随着训练样本的增加而减少．当样本比较多时，方差比较少，这时可以选择能力强的模型来减少偏差．

期望错误

![image-20210525153908610](/images/image-20210525153908610.png)

其中

![image-20210525153919761](/images/image-20210525153919761.png)

### **2.7** **评价指标**

**准确率** 最常用的评价指标为准确率（Accuracy）：

![image-20210525154620800](/images/image-20210525154620800.png)

**错误率** 和准确率相对应的就是错误率（Error Rate）：

![image-20210525154653777](/images/image-20210525154653777.png)

**精确率和召回率** 

准确率是所有类别整体性能的平均，如果希望对每个类都进行性能估计，就需要计算精确率（Precision）和召回率（Recall）．

1. 真正例（True Positive，TP）：一个样本的真实类别为𝑐并且模型正确地预测为类别𝑐．
2. 假负例（False Negative，FN）：一个样本的真实类别为𝑐，模型错误地预测为其他类．
3. 假正例（False Positive，FP）：一个样本的真实类别为其他类，模型错误地预测为类别𝑐
4. 真负例（True Negative，TN）：一个样本的真实类别为其他类，模型也预测为其他类

![image-20210525155039087](/images/image-20210525155039087.png)

**精确率（Precision）**，也叫精度或查准率，

类别 𝑐 的查准率是所有预测为类别𝑐的样本中预测正确的比例:

![image-20210525155122489](/images/image-20210525155122489.png)

召回率（Recall），也叫查全率，类别𝑐的查全率是所有真实标签为类别𝑐的样本中预测正确的比例：

![image-20210525155216428](/images/image-20210525155216428.png)

F值（F Measure）是一个综合指标，为精确率和召回率的调和平均：

![image-20210525155253112](/images/image-20210525155253112.png)

其中𝛽 用于平衡精确率和召回率的重要性，一般取值为1．𝛽 = 1时的F值称为F1值，是精确率和召回率的调和平均．

