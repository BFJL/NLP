# 神经网络与深度学习-邱锡鹏

# 第2章 机器学习概述

### 2.2.2.1 损失函数

**0-1** **损失函数**

![image-20210525142536974](/images/image-20210525142536974.png)

**平方损失函数**

![image-20210525142635658](/images/image-20210525142635658.png)

**交叉熵损失函数**

交叉熵损失函数（Cross-Entropy Loss Function）一般用于分类问题．假设样本的标签 𝑦 ∈ {1, ⋯ , 𝐶} 为离散的类别，模型 𝑓(𝒙; 𝜃) ∈ [0, 1]𝐶的输出为类别标签的条件概率分布，即

![image-20210525142805524](/images/image-20210525142805524.png)

并满足

![image-20210525142817757](/images/image-20210525142817757.png)

我们可以用一个𝐶 维的one-hot向量𝒚来表示样本标签．假设样本的标签为𝑘，那么标签向量𝒚只有第𝑘维的值为1，其余元素的值都为0．标签向量𝒚可以看作样本标签的真实条件概率分布𝑝𝑟 (𝒚|𝒙)，即第𝑐维（记为𝑦𝑐，1 ≤ 𝑐 ≤ 𝐶）是类别为 𝑐 的真实条件概率．假设样本的类别为 

𝑘，那么它属于第 𝑘 类的概率为 1，属于其他类的概率为0．对于两个概率分布，一般可以用交叉熵来衡量它们的差异． 标签的真实分布𝒚和模型预测分布𝑓(𝒙; 𝜃)之间的交叉熵为

![image-20210525142959827](/images/image-20210525142959827.png)

如果y为one-hot向量，也可以写为

![image-20210525143131780](/images/image-20210525143131780.png)

**Hinge** **损失函数**

对于二分类问题，假设 𝑦 的取值为 {−1, +1}，𝑓(𝒙; 𝜃) ∈ ℝ．Hinge损失函数（Hinge Loss Function）为

![image-20210525143215743](/images/image-20210525143215743.png)

### **2.2.2.2** **风险最小化准则**

经验风险（Empirical Risk），即在训练集上的平均损失：

![image-20210525143311725](/images/image-20210525143311725.png)

因此，一个切实可行的学习准则是找到一组参数𝜃∗ 使得经验风险最小，即 

![image-20210525143354450](/images/image-20210525143354450.png)

这就是经验风险最小化（Empirical Risk Minimization，ERM）准则。

**过拟合**

过拟合问题往往是由于训练数据少和噪声以及模型能力强等原因造成的。

为了解决过拟合问题，一般在经验风险最小化的基础上再引入参数的正则化

（Regularization）来限制模型能力，使其不要过度地最小化经验风险． 

这种准则就是结构风险最小化（Structure Risk Minimization，SRM）准则：

![image-20210525143612181](/images/image-20210525143612181.png)

其中‖𝜃‖是ℓ2 范数的正则化项，用来减少参数空间，避免过拟合；𝜆用来控制正则

化的强度。

**防止过拟合**

- 正则化
- 提前停止
- 限制模型的复杂度

和过拟合相反的一个概念是**欠拟合（Underfitting）**，即模型不能很好地拟

合训练数据，在训练集上的错误率比较高．欠拟合一般是由于模型能力不足造成

的．

### **2.2.3** **优化算法**

在确定了训练集 𝒟、假设空间 ℱ 以及学习准则后，如何找到最优的模型𝑓(𝒙, 𝜃∗) 就成了一个最优化（Optimization）问题．机器学习的训练过程其实就是最优化问题的求解过程．

### **2.2.3.1** **梯度下降法**

最简单、常用的优化算法就是梯度下降法，即首先初始化参数𝜃0，然后按下面的迭代公式来计算训练集𝒟 上风险函数的最小值：

![image-20210525144000493](/images/image-20210525144000493.png)

### **2.2.3.3** **随机梯度下降法**

梯度下降法中，目标函数是整个训练集上的风险函数，这种方式称为批量梯度下降法（Batch Gradient Descent，BGD）．批量梯度下降法在每次迭代时需要计算每个样本上损失函数的梯度并求和．当训练集中的样本数量𝑁 很大时，空间复杂度比较高，每次迭代的计算开销也很大．

为了减少每次迭代的计算复杂度，我们也可以在每次迭代时只采集一个样本，计算这个样本损失函数的梯度并更新参数，即随机梯度下降法（Stochastic Gradient Descent，SGD）．

随机梯度下降法也叫作增量梯度下降法． 

批量梯度下降和随机梯度下降之间的区别在于，每次迭代的优化目标是对所有样本的平均损失函数还是对单个样本的损失函数．由于随机梯度下降实现简单，收敛速度也非常快，因此使用非常广泛．随机梯度下降相当于在批量梯度下降的梯度上引入了随机噪声．在非凸优化问题中，随机梯度下降更容易逃离局部最优点。

### **2.2.3.4** **小批量梯度下降法**

随机梯度下降法的一个缺点是无法充分利用计算机的并行计算能力．小批量梯度下降法（Mini-Batch Gradient Descent）是批量梯度下降和随机梯度下降的折中．每次迭代时，我们随机选取一小部分训练样本来计算梯度并更新参数，这样既可以兼顾随机梯度下降法的优点，也可以提高训练效率．

![image-20210525144517925](/images/image-20210525144517925.png)

### **2.3.1** **参数学习**

### **2.3.1.1** **经验风险最小化**

于线性回归的标签𝑦和模型输出都为连续的实数值， 因此平方损失函数非常合适衡量真实标签和预测标签之间的差异．

根据经验风险最小化准则，训练集𝒟 上的经验风险定义为

![image-20210525151515960](/images/image-20210525151515960.png)

风险函数ℛ(𝒘)是关于𝒘的凸函数，其对𝒘的偏导数为

![image-20210525151539321](/images/image-20210525151539321.png)

![image-20210525151555999](/images/image-20210525151555999.png)

这种求解线性回归参数的方法也叫最小二乘法（Least Square Method，LSM）．

