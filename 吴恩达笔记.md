# 笔记

RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：

1. 时间片 ![[公式]](https://www.zhihu.com/equation?tex=t) 的计算依赖 ![[公式]](https://www.zhihu.com/equation?tex=t-1) 时刻的计算结果，这样限制了模型的并行能力；
2. 顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。

Transformer的提出解决了上面两个问题，首先它使用了Attention机制，将序列中的任意两个位置之间的距离是缩小为一个常量；其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。

验证集和测试集的数据来自同一分布。

测试集的目的是对最终所选定的神经网络系统做出无偏估计，如果不需要无偏估计，也可以不设置测试集。

![img](images/e0ec4205933b7c2a9eaed9fbaa8d4afc.png)

假设这就是数据集，如果给这个数据集拟合一条直线，可能得到一个逻辑回归拟合，但它并不能很好地拟合该数据，这是高偏差（**high bias**）的情况，我们称为“欠拟合”（**underfitting**

相反的如果我们拟合一个非常复杂的分类器，比如深度神经网络或含有隐藏单元的神经网络，可能就非常适用于这个数据集，但是这看起来也不是一种很好的拟合方式分类器方差较高（**high variance**），数据过度拟合（**overfitting**）

过拟合问题——高方差，有两个解决方法，一个是正则化，另一个是准备更多的数据。

L1 正则化

L2 正则化 有时被称为“权重衰减”的原因

**Dropout**（随机失活）正则化，我们来看看它的工作原理。

**dropout**呢？方法有几种，接下来我要讲的是最常用的方法，即**inverted dropout**（反向随机失活）直观理解，不要依赖于任何一个特征

**dropout**是一种正则化方法，它有助于预防过拟合，因此除非算法过拟合，不然我是不会使用**dropout**的。

**dropout**一大缺点就是代价函数不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。

### 其他正则化方法（Other regularization methods）：

数据扩增

**early stopping**代表提早停止训练神经网络：**early stopping**的主要缺点就是你不能独立地处理这两个问题，因为提早停止梯度下降，也就是停止了优化代价函数，因为现在你不再尝试降低代价函数，所以代价函数的值可能不够小，同时你又希望不出现过拟合，你没有采取不同的方式来解决这两个问题，而是用一种方法同时解决两个问题，这样做的结果是我要考虑的东西变得更复杂。

如果不用**early stopping**，另一种方法就是正则化，训练神经网络的时间就可能很长。我发现，这导致超级参数搜索空间更容易分解，也更容易搜索，但是缺点在于，你必须尝试很多正则化参数的值，这也导致搜索大量值的计算代价太高。

**Early stopping**的优点是，只运行一次梯度下降，你可以找出的较小值，中间值和较大值，而无需尝试正则化超级参数的很多值。

归一化需要两个步骤：

1. 零均值
2. 归一化方差；

你现在对梯度消失或爆炸问题以及如何为权重初始化合理值已经有了一个直观认识，希望你设置的权重矩阵既不会增长过快，也不会太快下降到0，从而训练出一个权重或梯度不会增长或消失过快的深度网络。我们在训练深度网络时，这也是一个加快训练速度的技巧。



双边误差来判断别人给你的函数，是否正确实现了函数的偏导，现在我们可以使用这个方法来检验反向传播是否得以正确实施

# 统计学中被称为指数加权移动平均值，我们就简称为指数加权平均数

### 动量梯度下降法（Gradient descent with Momentum）

**RMSprop**的算法，全称是**root mean square prop**算法，它也可以加速梯度下降

### Adam 优化算法(Adam optimization algorithm) **Nesterov**

基本上就是将**Momentum**和**RMSprop**结合在一起，那么来看看如何使用**Adam**算法。

### 学习率衰减(Learning rate decay)

### 归一化网络的激活函数  **Batch**归一化

### Batch Norm

**Batch**归一化起的作用的原因，直观的一点就是，它在做类似的工作，但不仅仅对于这里的输入值，还有隐藏单元的值，这只是**Batch**归一化作用的冰山一角，还有些深层的原理，它会有助于你对**Batch**归一化的作用有更深的理解，让我们一起来看看吧。

**Batch**归一化有效的第二个原因是，它可以使权重比你的网络更滞后或更深层，比如，第10层的权重更能经受得住变化，相比于神经网络中前层的权重，

### Softmax 回归（Softmax regression）

查准率（**precision**）和查全率（**recall**）。

查准率的定义是在你的分类器标记为猫的例子中，有多少真的是猫。所以如果分类器有95%的查准率，这意味着你的分类器说这图有猫的时候，有95%的机会真的是猫。

查全率就是，对于所有真猫的图片，你的分类器正确识别出了多少百分比。实际为猫的图片中，有多少被系统识别出来？如果分类器查全率是90%，这意味着对于所有的图像，比如说你的开发集都是真的猫图，分类器准确地分辨出了其中的90%。

结合查准率和查全率的标准方法是所谓的分数，分数的细节并不重要。但非正式的，你可以认为这是查准率和查全率的平均值。

这个函数叫做查准率和查全率的调和平均数

开发集和测试集为什么必须来自同一分布

现在流行的是把大量数据分到训练集，然后少量数据分到开发集和测试集，特别是当你有一个非常大的数据集时。

端到端学习到底是什么呢？简而言之，以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理。那么端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它。

词嵌入还有一个迷人的特性就是它还能帮助实现类比推理

**Word2Vec**

 **Skip-Gram**

**CBOW**是从原始语句推测目标字词；而**Skip-Gram**正好相反，是从目标字词推测出原始语句。**CBOW**对小型数据库比较合适，而**Skip-Gram**在大型语料中表现更好。

**Skip-Gram**模型，关键问题在于**softmax**这个步骤的计算成本非常昂贵，因为它需要在分母里对词汇表中所有词求和。通常情况下，**Skip-Gram**模型用到更多点。

那么如何选取k？**Mikolov**等人推荐小数据集的话，k从5到20比较好。如果你的数据集很大，就选的小一点。对于更大的数据集就等于2到5，数据集越小就越大。那么在这个例子中，我们就用k=4。

### GloVe 词向量（GloVe Word Vectors）

现在机器学习和人工智能算法正渐渐地被信任用以辅助或是制定极其重要的决策，因此我们想尽可能地确保它们不受非预期形式偏见影响，比如说性别歧视、种族歧视等等。本节视频中我会向你展示词嵌入中一些有关减少或是消除这些形式的偏见的办法。

束搜索(**Beam Search**) 长度归一化（**Length normalization**）

****

贪心搜索(**Greedy Search**)

### 注意力模型